{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "verify.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdDvdc_7ns8j"
      },
      "outputs": [],
      "source": [
        "!pip install gym-jsbsim\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import gym_jsbsim\n",
        "import numpy as np\n",
        "from stable_baselines3 import DQN, A2C, DDPG, HER, PPO, SAC, TD3\n",
        "from stable_baselines3.common.env_checker import check_env"
      ],
      "metadata": {
        "id": "00zFRzJNn2ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Obs_TupleToBoxWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        low = np.empty(shape=(0,))\n",
        "        high = np.empty(shape=(0,))\n",
        "        for i in env.observation_space:\n",
        "          low = np.concatenate([low,i.low])\n",
        "          high = np.concatenate([high,i.high])\n",
        "        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=\"float\")\n",
        "        \n",
        "    \n",
        "    def observation(self, obs):\n",
        "        new_obs = np.empty(shape=(0,))\n",
        "        for i in obs:\n",
        "          new_obs = np.concatenate([new_obs,i])\n",
        "        return new_obs"
      ],
      "metadata": {
        "id": "H8G_GNWun5jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Act_TupleToBoxWrapper(gym.ActionWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        low = np.empty(shape=(0,))\n",
        "        high = np.empty(shape=(0,))\n",
        "        for i in env.action_space:\n",
        "          low = np.concatenate([low,i.low])\n",
        "          high = np.concatenate([high,i.high])\n",
        "        self.action_space = gym.spaces.Box(low=low, high=high, dtype=\"float\")\n",
        "        \n",
        "    \n",
        "    def action(self, act):\n",
        "#        print(act)\n",
        "#        new_act = np.empty(shape=(0,))\n",
        "#        for i in act:\n",
        "#          new_act = np.concatenate([new_act,i])\n",
        "        return act"
      ],
      "metadata": {
        "id": "GDU10H3_n-Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"GymJsbsim-HeadingControlTask-v0\")\n",
        "#print(env.observation_space)\n",
        "#print(env.observation_space.sample())\n",
        "#print(env.action_space)\n",
        "#print(env.action_space.sample())\n",
        "#print(\"\")\n",
        "env = Obs_TupleToBoxWrapper(env)\n",
        "#print(env.observation_space)\n",
        "#print(env.observation_space.sample())\n",
        "#print(env.action_space)\n",
        "#print(env.action_space.sample())\n",
        "#print(\"\")\n",
        "env = Act_TupleToBoxWrapper(env)\n",
        "#print(env.observation_space)\n",
        "#print(env.observation_space.sample())\n",
        "#print(env.action_space)\n",
        "#print(env.action_space.sample())"
      ],
      "metadata": {
        "id": "FS9S6ykmoBNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = PPO.load(\"./model1.zip\", env=env)\n",
        "reward_liste = []\n",
        "for episode in range(100):\n",
        "  episode_reward = 0\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action, _states = model1.predict(obs, deterministic=True)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "  reward_liste.append(episode_reward)\n",
        "  print(\"episode\", episode, \" ---> reward = \", episode_reward)\n",
        "print(\"nb steps = \", len(reward_liste), \"sum reward =\", sum(reward_liste), \", mean reward =\", sum(reward_liste)/len(reward_liste))"
      ],
      "metadata": {
        "id": "D2T5HyV2oE-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = PPO.load(\"./model2.zip\", env=env)\n",
        "reward_liste = []\n",
        "for episode in range(100):\n",
        "  episode_reward = 0\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  while not done:\n",
        "    action, _states = model2.predict(obs, deterministic=True)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    episode_reward += reward\n",
        "  reward_liste.append(episode_reward)\n",
        "  print(\"episode\", episode, \" ---> reward = \", episode_reward)\n",
        "print(\"nb steps = \", len(reward_liste), \"sum reward =\", sum(reward_liste), \", mean reward =\", sum(reward_liste)/len(reward_liste))"
      ],
      "metadata": {
        "id": "TdTzRub4obl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}